"""
download_range.py â€“ numbered page crawler with curlâ€‘cffi â‰¥â€¯0.6
--------------------------------------------------------------
â€¢ PageÂ 1 is fetched as the bare URL when no {n} placeholder is present.
â€¢ Any pages that fail to download are listed at the end.
# 1. First page is bare URL, others get ?page=N
python download_range.py "https://example.com" 1 20 pages/

# 2. Placeholder variant â€“ you decide how pageâ€¯1 looks
python download_range.py "https://example.com/products?page={n}" 1 10 out/ -t 12
"""

from __future__ import annotations
import argparse, concurrent.futures, pathlib, urllib.parse as up
import curl_cffi
from tqdm import tqdm


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ URL generation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_url(base: str, n: int) -> str:
    """
    Make the URL for page *n*.

    â€¢ With a `{n}` placeholder â†’ simple substitution.
    â€¢ Without `{n}` â†’ page 1 is `base`, others get â€œpage=nâ€ appended.
    """
    if "{n}" in base:
        return base.replace("{n}", str(n))

    if n == 1:  # bare URL for first page
        return base

    parsed = up.urlsplit(base)
    query = up.parse_qs(parsed.query, keep_blank_values=True)
    query["page"] = [str(n)]
    new_query = up.urlencode(query, doseq=True)
    return up.urlunsplit(parsed._replace(query=new_query))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ filename helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def make_filename(url: str, out_dir: pathlib.Path) -> pathlib.Path:
    """Turn https://example.com?a=1&page=3 into example.com_a-1_page-3.html"""
    parsed = up.urlsplit(url)
    stem = (parsed.netloc + parsed.path).replace("/", "_").strip("_") or "index"

    qs = up.parse_qs(parsed.query)
    page = qs.get("page", [""])[0]
    if page:
        stem += f"_page-{page}"

    return out_dir / f"{stem}.html"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ download worker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_and_save(
    url: str,
    out_dir: pathlib.Path,
    *,
    impersonate: str,
    proxies: dict | None,
    timeout: int,
) -> str:
    r = curl_cffi.get(url, impersonate=impersonate, proxies=proxies, timeout=timeout)
    r.raise_for_status()
    dst = make_filename(url, out_dir)
    dst.parent.mkdir(parents=True, exist_ok=True)
    dst.write_bytes(r.content)
    return str(dst)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    ap = argparse.ArgumentParser(description="Download numbered pages with curlâ€‘cffi")
    ap.add_argument("base_url", help='Base URL ("{n}" placeholder optional)')
    ap.add_argument("start", type=int, help="First page number (inclusive)")
    ap.add_argument("end", type=int, help="Last page number (inclusive)")
    ap.add_argument("out_dir", type=pathlib.Path, help="Directory to save pages")
    ap.add_argument(
        "-t", "--threads", type=int, default=8, help="Parallel workers (default 8)"
    )
    ap.add_argument(
        "-i",
        "--impersonate",
        default="chrome",
        help='Impersonation string (default "chrome")',
    )
    ap.add_argument(
        "-p",
        "--proxy",
        default=None,
        help='HTTPS/SOCKS proxy, e.g. "socks://127.0.0.1:9050"',
    )
    ap.add_argument(
        "--timeout", type=int, default=30, help="Request timeout seconds (default 30)"
    )
    args = ap.parse_args()

    pages = range(args.start, args.end + 1)
    urls = {n: build_url(args.base_url, n) for n in pages}

    proxies = {"https": args.proxy} if args.proxy else None
    args.out_dir.mkdir(parents=True, exist_ok=True)

    failed: list[int] = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=args.threads) as pool:
        futures = {
            pool.submit(
                fetch_and_save,
                url,
                args.out_dir,
                impersonate=args.impersonate,
                proxies=proxies,
                timeout=args.timeout,
            ): n
            for n, url in urls.items()
        }
        for f in tqdm(
            concurrent.futures.as_completed(futures),
            total=len(futures),
            desc="Downloading",
        ):
            n = futures[f]
            try:
                local = f.result()
                tqdm.write(f"âœ“ page {n} â†’ {local}")
            except Exception as e:
                failed.append(n)
                tqdm.write(f"âœ— page {n} ({e})")

    # â”€â”€â”€ summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if failed:
        failed.sort()
        print(f"\nFailed pages: {', '.join(map(str, failed))}")
    else:
        print("\nAll pages downloaded successfully ðŸŽ‰")


if __name__ == "__main__":
    main()
